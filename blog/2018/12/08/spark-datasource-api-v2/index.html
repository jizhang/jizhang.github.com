<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Spark DataSource API V2 | Ji ZHANG&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="From Spark 1.3, the team introduced a data source API to help quickly integrating various input formats with Spark SQL. But eventually this version of API became insufficient and the team needed to ad">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark DataSource API V2">
<meta property="og:url" content="http://shzhangji.com/blog/2018/12/08/spark-datasource-api-v2/index.html">
<meta property="og:site_name" content="Ji ZHANG&#39;s Blog">
<meta property="og:description" content="From Spark 1.3, the team introduced a data source API to help quickly integrating various input formats with Spark SQL. But eventually this version of API became insufficient and the team needed to ad">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-12-08T10:23:11.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark DataSource API V2">
<meta name="twitter:description" content="From Spark 1.3, the team introduced a data source API to help quickly integrating various input formats with Spark SQL. But eventually this version of API became insufficient and the team needed to ad">
<meta name="twitter:creator" content="@zjerryj">
<link rel="publisher" href="zhangji87@gmail.com">
  
    <link rel="alternate" href="/atom.xml" title="Ji ZHANG&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link rel="stylesheet" href="/css/source-code-pro.css">
  
  <link rel="stylesheet" href="/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-37223379-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ji ZHANG&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">If I rest, I rust.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/categories/Big-Data">Big Data</a>
        
          <a class="main-nav-link" href="/categories/Programming">Programming</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
        <a class="main-nav-link" href="http://shzhangji.com/cnblogs"><img src="/images/cnblogs.png"></a>
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shzhangji.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-spark-datasource-api-v2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2018/12/08/spark-datasource-api-v2/" class="article-date">
  <time datetime="2018-12-08T10:23:11.000Z" itemprop="datePublished">2018-12-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Big-Data/">Big Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark DataSource API V2
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>From Spark 1.3, the team introduced a data source API to help quickly integrating various input formats with Spark SQL. But eventually this version of API became insufficient and the team needed to add a lot of internal codes to provide more efficient solutions for Spark SQL data sources. So in Spark 2.3, the second version of data source API is out, which is supposed to overcome the limitations of the previous version. In this article, I will demonstrate how to implement custom data source for Spark SQL in both V1 and V2 API, to help understanding their differences and the new API’s advantages.</p>
<h2 id="DataSource-V1-API"><a href="#DataSource-V1-API" class="headerlink" title="DataSource V1 API"></a>DataSource V1 API</h2><p>V1 API provides a set of abstract classes and traits. They are located in <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala" target="_blank" rel="noopener">spark/sql/sources/interfaces.scala</a>. Some basic APIs are:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>A <code>RelationProvider</code> defines a class that can create a relational data source for Spark SQL to manipulate with. It can initialize itself with provided options, such as file path or authentication. <code>BaseRelation</code> is used to define the data schema, which can be loaded from database, Parquet file, or specified by the user. This class also needs to mix-in one of the <code>Scan</code> traits, implements the <code>buildScan</code> method, and returns an RDD.</p>
<a id="more"></a>
<h3 id="JdbcSourceV1"><a href="#JdbcSourceV1" class="headerlink" title="JdbcSourceV1"></a>JdbcSourceV1</h3><p>Now we use V1 API to implement a JDBC data source. For simplicity, the table schema is hard coded, and it only supports full table scan. Complete example can be found on GitHub (<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/datasource/JdbcExampleV1.scala" target="_blank" rel="noopener">link</a>), while the sample data is in <a href="https://github.com/jizhang/spark-sandbox/blob/master/data/employee.sql" target="_blank" rel="noopener">here</a>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcSourceV1</span> <span class="keyword">extends</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcRelationV1</span>(parameters(<span class="string">"url"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationV1</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Seq</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"emp_name"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(url)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRDD</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(): <span class="type">Iterator</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    <span class="keyword">val</span> stmt = conn.prepareStatement(<span class="string">"SELECT * FROM employee"</span>)</span><br><span class="line">    <span class="keyword">val</span> rs = stmt.executeQuery()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Row</span>] &#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = rs.next()</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>: <span class="type">Row</span> = <span class="type">Row</span>(rs.getInt(<span class="string">"id"</span>), rs.getString(<span class="string">"emp_name"</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The actual data reading happens in <code>JdbcRDD#compute</code>. It receives the connection options, possibly with pruned column list and where conditions, executes the query, and returns an iterator of <code>Row</code> objects, correspondent to the defined schema. Now we can create a <code>DataFrame</code> from this custom data source.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read</span><br><span class="line">  .format(<span class="string">"JdbcSourceV2"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost/spark"</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<p>The outputs are:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- emp_name: string (nullable = true)</span><br><span class="line"> |-- dep_name: string (nullable = true)</span><br><span class="line"> |-- salary: decimal(7,2) (nullable = true)</span><br><span class="line"> |-- age: decimal(3,0) (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+--------+----------+-------+---+</span><br><span class="line">| id|emp_name|  dep_name| salary|age|</span><br><span class="line">+---+--------+----------+-------+---+</span><br><span class="line">|  1| Matthew|Management|4500.00| 55|</span><br><span class="line">|  2|  Olivia|Management|4400.00| 61|</span><br><span class="line">|  3|   Grace|Management|4000.00| 42|</span><br><span class="line">|  4|     Jim|Production|3700.00| 35|</span><br><span class="line">|  5|   Alice|Production|3500.00| 24|</span><br><span class="line">+---+--------+----------+-------+---+</span><br></pre></td></tr></table></figure>
<h3 id="Limitations-of-V1-API"><a href="#Limitations-of-V1-API" class="headerlink" title="Limitations of V1 API"></a>Limitations of V1 API</h3><p>As we can see, V1 API is quite straightforward and can meet the initial requirements of Spark SQL use cases. But as Spark moves forward, V1 API starts to show its limitations.</p>
<h4 id="Coupled-with-Higher-Level-API"><a href="#Coupled-with-Higher-Level-API" class="headerlink" title="Coupled with Higher Level API"></a>Coupled with Higher Level API</h4><p><code>createRelation</code> accepts <code>SQLContext</code> as parameter; <code>buildScan</code> returns <code>RDD</code> of <code>Row</code>; and when implementing writable data source, the <code>insert</code> method accepts <code>DataFrame</code> type.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">InsertableRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>These classes are of higher level of Spark API, and some of them have already upgraded, like <code>SQLContext</code> is superceded by <code>SparkSession</code>, and <code>DataFrame</code> is now an alias of <code>Dataset[Row]</code>. Data sources should not be required to reflect these changes.</p>
<h4 id="Hard-to-Add-New-Push-Down-Operators"><a href="#Hard-to-Add-New-Push-Down-Operators" class="headerlink" title="Hard to Add New Push Down Operators"></a>Hard to Add New Push Down Operators</h4><p>Besides <code>TableScan</code>, V1 API provides <code>PrunedScan</code> to eliminate unnecessary columns, and <code>PrunedFilteredScan</code> to push predicates down to data source. In <code>JdbcSourceV1</code>, they are reflected in the SQL statement.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationV1</span> <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">PrunedFilteredScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]) = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcRDD</span>(requiredColumns, filters)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRDD</span>(<span class="params">columns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]</span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">val</span> wheres = filters.flatMap &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">EqualTo</span>(attribute, value) =&gt; <span class="type">Some</span>(<span class="string">s"<span class="subst">$attribute</span> = '<span class="subst">$value</span>'"</span>)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">s"SELECT <span class="subst">$&#123;columns.mkString(", ")&#125;</span> FROM employee WHERE <span class="subst">$&#123;wheres.mkString(" AND ")&#125;</span>"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>What if we need to push down a new operator like <code>limit</code>? It will introduce a whole new set of <code>Scan</code> traits.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">LimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedLimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedFilteredLimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>], limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Hard-to-Pass-Partition-Info"><a href="#Hard-to-Pass-Partition-Info" class="headerlink" title="Hard to Pass Partition Info"></a>Hard to Pass Partition Info</h4><p>For data sources that support partitioning like HDFS and Kafka, V1 API does not provide native support for partitioning and data locality. We need to achieve this by extending the RDD class. For instance, some Kafka topic contains several partitions, and we want the data reading task to be run on the servers where leader brokers reside.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaPartition</span>(<span class="params">partitionId: <span class="type">Int</span>, leaderHost: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Partition</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">index</span></span>: <span class="type">Int</span> = partitionId</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRDD</span>(<span class="params">sc: <span class="type">SparkContext</span></span>) <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">Row</span>](<span class="params">sc, <span class="type">Nil</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = <span class="type">Array</span>(</span><br><span class="line">    <span class="comment">// populate with Kafka PartitionInfo</span></span><br><span class="line">    <span class="type">KafkaPartition</span>(<span class="number">0</span>, <span class="string">"broker_0"</span>),</span><br><span class="line">    <span class="type">KafkaPartition</span>(<span class="number">1</span>, <span class="string">"broker_1"</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Seq</span>(</span><br><span class="line">    split.asInstanceOf[<span class="type">KafkaPartition</span>].leaderHost</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Besides, some database like Cassandra distributes data by primary key. If the query pipeline contains grouping on the columns, this information can be used by the optimizer to avoid shuffling. V2 API supports this with a dedicated trait.</p>
<h4 id="Lack-of-Transactional-Writing"><a href="#Lack-of-Transactional-Writing" class="headerlink" title="Lack of Transactional Writing"></a>Lack of Transactional Writing</h4><p>Spark tasks may fail, and with V1 API there will be partially written data. For file systems like HDFS, we can put a <code>_SUCCESS</code> file in the output directory to indicate if the job finishes successfully, but this process needs to be implemented by users, while V2 API provides explicit interfaces to support transactional writing.</p>
<h4 id="Lack-of-Columnar-and-Streaming-Support"><a href="#Lack-of-Columnar-and-Streaming-Support" class="headerlink" title="Lack of Columnar and Streaming Support"></a>Lack of Columnar and Streaming Support</h4><p>Columnar data and stream processing are both added to Spark SQL without using V1 API. Current implementations like <code>ParquetFileFormat</code> and <code>KafkaSource</code> are written in dedicated codes with internal APIs. These features are also addressed by V2 API.</p>
<h2 id="DataSource-V2-API"><a href="#DataSource-V2-API" class="headerlink" title="DataSource V2 API"></a>DataSource V2 API</h2><p>V2 API starts with a marker interface <code>DataSourceV2</code>. The class needs to be mixed-in with either <code>ReadSupport</code> or <code>WriteSupport</code>. <code>ReadSupport</code> interface, for instance, creates a <code>DataSourceReader</code> with initialization options; <code>DataSourceReader</code> reads schema of the data source, and returns a list of <code>DataReaderFactory</code>; the factory will create the actual <code>DataReader</code>, which works like an iterator. Besides, <code>DataSourceReader</code> can mix-in various <code>Support</code> interfaces, to apply query optimizations like operator push-down and columnar scan. For <code>WriteSupport</code> interfaces, the hierarchy is similar. All of them are written in Java for better interoperability.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceV2</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ReadSupport</span> <span class="keyword">extends</span> <span class="title">DataSourceV2</span> </span>&#123;</span><br><span class="line">  <span class="function">DataSourceReader <span class="title">createReader</span><span class="params">(DataSourceOptions options)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function">StructType <span class="title">readSchema</span><span class="params">()</span></span>;</span><br><span class="line">  List&lt;DataReaderFactory&lt;Row&gt;&gt; createDataReaderFactories();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SupportsPushDownRequiredColumns</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">pruneColumns</span><span class="params">(StructType requiredSchema)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataReaderFactory</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function">DataReader&lt;T&gt; <span class="title">createDataReader</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataReader</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">boolean</span> <span class="title">next</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function">T <span class="title">get</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You may notice that <code>DataSourceReader#createDataReaderFactories</code> still relies on <code>Row</code> class, because currently only <code>Row</code> is supported, and V2 API is still marked as <code>Evolving</code>.</p>
<h3 id="JdbcSourceV2"><a href="#JdbcSourceV2" class="headerlink" title="JdbcSourceV2"></a>JdbcSourceV2</h3><p>Let us rewrite the JDBC data source with V2 API. The following is an abridged example of full table scan. Complete code can be found on GitHub (<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/datasource/JdbcExampleV2.scala" target="_blank" rel="noopener">link</a>).</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readSchema</span> </span>= <span class="type">StructType</span>(<span class="type">Seq</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"emp_name"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(url)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataReader</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">DataReader</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> rs: <span class="type">ResultSet</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">if</span> (rs == <span class="literal">null</span>) &#123;</span><br><span class="line">      conn = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">      <span class="keyword">val</span> stmt = conn.prepareStatement(<span class="string">"SELECT * FROM employee"</span>)</span><br><span class="line">      rs = stmt.executeQuery()</span><br><span class="line">    &#125;</span><br><span class="line">    rs.next()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>() = <span class="type">Row</span>(rs.getInt(<span class="string">"id"</span>), rs.getString(<span class="string">"emp_name"</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Prune-Columns"><a href="#Prune-Columns" class="headerlink" title="Prune Columns"></a>Prune Columns</h4><p><code>DataSourceReader</code> can mix-in the <code>SupportsPushDownRequiredColumns</code> trait. Spark will invoke the <code>pruneColumns</code> method with required <code>StructType</code>, and <code>DataSourceReader</code> can pass it to underlying <code>DataReader</code>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownRequiredColumns</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> requiredSchema = <span class="type">JdbcSourceV2</span>.schema</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pruneColumns</span></span>(requiredSchema: <span class="type">StructType</span>)  = &#123;</span><br><span class="line">    <span class="keyword">this</span>.requiredSchema = requiredSchema</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">val</span> columns = requiredSchema.fields.map(_.name)</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(columns)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>We can examine the execution plan with <code>df.explain(true)</code>. For instance, the optimized logical plan of query <code>SELECT emp_name, age FROM employee</code> shows column pruning is pushed down to the data source.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">emp_name: string, age: decimal(3,0)</span><br><span class="line">Project [emp_name#1, age#4]</span><br><span class="line">+- SubqueryAlias employee</span><br><span class="line">   +- DataSourceV2Relation [id#0, emp_name#1, dep_name#2, salary#3, age#4], datasource.JdbcDataSourceReader@15ceeb42</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Project [emp_name#1, age#4]</span><br><span class="line">+- DataSourceV2Relation [emp_name#1, age#4], datasource.JdbcDataSourceReader@15ceeb42</span><br></pre></td></tr></table></figure>
<h4 id="Push-Down-Filters"><a href="#Push-Down-Filters" class="headerlink" title="Push Down Filters"></a>Push Down Filters</h4><p>Similarly, with <code>SupportsPushDownFilters</code>, we can add where conditions to the underlying SQL query.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownFilters</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> filters = <span class="type">Array</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">  <span class="keyword">var</span> wheres = <span class="type">Array</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pushFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> supported = <span class="type">ListBuffer</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">    <span class="keyword">val</span> unsupported = <span class="type">ListBuffer</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">    <span class="keyword">val</span> wheres = <span class="type">ListBuffer</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">    filters.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> filter: <span class="type">EqualTo</span> =&gt; &#123;</span><br><span class="line">        supported += filter</span><br><span class="line">        wheres += <span class="string">s"<span class="subst">$&#123;filter.attribute&#125;</span> = '<span class="subst">$&#123;filter.value&#125;</span>'"</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> filter =&gt; unsupported += filter</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.filters = supported.toArray</span><br><span class="line">    <span class="keyword">this</span>.wheres = wheres.toArray</span><br><span class="line">    unsupported.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pushedFilters</span> </span>= filters</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(wheres)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Multiple-Partitions"><a href="#Multiple-Partitions" class="headerlink" title="Multiple Partitions"></a>Multiple Partitions</h4><p><code>createDataReaderFactories</code> returns a list. Each reader will output data for an RDD partition. Say we want to parallelize the data reading tasks, we can divide the records into two parts, according to primary key ranges.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">  <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">6</span>), (<span class="number">7</span>, <span class="number">11</span>)).map &#123; <span class="keyword">case</span> (minId, maxId) =&gt;</span><br><span class="line">    <span class="keyword">val</span> partition = <span class="string">s"id BETWEEN <span class="subst">$minId</span> AND <span class="subst">$maxId</span>"</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(partition)</span><br><span class="line">  &#125;.asJava</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Transactional-Write"><a href="#Transactional-Write" class="headerlink" title="Transactional Write"></a>Transactional Write</h3><p>V2 API provides two sets of <code>commit</code> / <code>abort</code> methods to implement transactional writes.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceWriter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">(WriterCommitMessage[] messages)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">abort</span><span class="params">(WriterCommitMessage[] messages)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataWriter</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">write</span><span class="params">(T record)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">  <span class="function">WriterCommitMessage <span class="title">commit</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">abort</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>DataSourceWriter</code> is running on Spark driver, <code>DataWriter</code> on executor. When <code>DataWriter</code> succeeds in writing, it sends commit message to driver, and after <code>DataSourceWriter</code> collects all writers’ commit messages, it will do the final commit. If the writer task fails, <code>abort</code> will be called, and a new task will be retried. When the retries hit the maximum, <code>abort</code> will be called on all tasks.</p>
<h3 id="Columnar-and-Streaming-Support"><a href="#Columnar-and-Streaming-Support" class="headerlink" title="Columnar and Streaming Support"></a>Columnar and Streaming Support</h3><p>These features are currently still in experimental status and there is no concrete implementation yet. Briefly, <code>DataSourceReader</code> can mix-in <code>SupportsScanColumnarBatch</code> trait and creates <code>DataReaderFactory</code> that handles <code>ColumnarBatch</code>, an interface that Spark uses to represent columnar data. For streaming support, there are <code>MicroBatchReader</code> and <code>ContinuousReader</code> traits. One can refer to the <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/test/scala/org/apache/spark/sql/sources/v2/DataSourceV2Suite.scala" target="_blank" rel="noopener">unit tests</a> for more details.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://blog.madhukaraphatak.com/spark-datasource-v2-part-1/" target="_blank" rel="noopener">http://blog.madhukaraphatak.com/spark-datasource-v2-part-1/</a></li>
<li><a href="https://databricks.com/session/apache-spark-data-source-v2" target="_blank" rel="noopener">https://databricks.com/session/apache-spark-data-source-v2</a></li>
<li><a href="https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html</a></li>
<li><a href="https://developer.ibm.com/code/2018/04/16/introducing-apache-spark-data-sources-api-v2/" target="_blank" rel="noopener">https://developer.ibm.com/code/2018/04/16/introducing-apache-spark-data-sources-api-v2/</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shzhangji.com/blog/2018/12/08/spark-datasource-api-v2/" data-id="cjwqd7z1u0025mgjkj9jhm68n" class="article-share-link">Share</a>
      
        <a href="http://shzhangji.com/blog/2018/12/08/spark-datasource-api-v2/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Real-time Exactly-once ETL with Apache Flink
        
      </div>
    </a>
  
  
    <a href="/blog/2018/10/03/flume-source-code-hdfs-sink/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Flume Source Code: HDFS Sink</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/analytics/" style="font-size: 15px;">analytics</a> <a href="/tags/apache-beam/" style="font-size: 10px;">apache beam</a> <a href="/tags/canal/" style="font-size: 10px;">canal</a> <a href="/tags/clojure/" style="font-size: 10px;">clojure</a> <a href="/tags/crossfilter/" style="font-size: 10px;">crossfilter</a> <a href="/tags/dc-js/" style="font-size: 10px;">dc.js</a> <a href="/tags/eclipse/" style="font-size: 10px;">eclipse</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/es6/" style="font-size: 10px;">es6</a> <a href="/tags/eslint/" style="font-size: 10px;">eslint</a> <a href="/tags/etl/" style="font-size: 13.33px;">etl</a> <a href="/tags/flink/" style="font-size: 10px;">flink</a> <a href="/tags/flume/" style="font-size: 13.33px;">flume</a> <a href="/tags/frontend/" style="font-size: 15px;">frontend</a> <a href="/tags/functional-programming/" style="font-size: 10px;">functional programming</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 10px;">hbase</a> <a href="/tags/hdfs/" style="font-size: 11.67px;">hdfs</a> <a href="/tags/hive/" style="font-size: 11.67px;">hive</a> <a href="/tags/java/" style="font-size: 18.33px;">java</a> <a href="/tags/javascript/" style="font-size: 16.67px;">javascript</a> <a href="/tags/kafka/" style="font-size: 11.67px;">kafka</a> <a href="/tags/lodash/" style="font-size: 11.67px;">lodash</a> <a href="/tags/machine-learning/" style="font-size: 10px;">machine learning</a> <a href="/tags/mapreduce/" style="font-size: 10px;">mapreduce</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/ops/" style="font-size: 10px;">ops</a> <a href="/tags/pandas/" style="font-size: 11.67px;">pandas</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/react/" style="font-size: 10px;">react</a> <a href="/tags/restful/" style="font-size: 10px;">restful</a> <a href="/tags/scala/" style="font-size: 11.67px;">scala</a> <a href="/tags/scalatra/" style="font-size: 10px;">scalatra</a> <a href="/tags/source-code/" style="font-size: 10px;">source code</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/spark-streaming/" style="font-size: 10px;">spark streaming</a> <a href="/tags/spring/" style="font-size: 10px;">spring</a> <a href="/tags/sql/" style="font-size: 11.67px;">sql</a> <a href="/tags/stream-processing/" style="font-size: 13.33px;">stream processing</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/thrift/" style="font-size: 10px;">thrift</a> <a href="/tags/vue/" style="font-size: 10px;">vue</a> <a href="/tags/vuex/" style="font-size: 10px;">vuex</a> <a href="/tags/webjars/" style="font-size: 10px;">webjars</a> <a href="/tags/websocket/" style="font-size: 10px;">websocket</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/05/">May 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">April 2013</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2019/06/10/understanding-hive-acid-transactional-table/">Understanding Hive ACID Transactional Table</a>
          </li>
        
          <li>
            <a href="/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/">Real-time Exactly-once ETL with Apache Flink</a>
          </li>
        
          <li>
            <a href="/blog/2018/12/08/spark-datasource-api-v2/">Spark DataSource API V2</a>
          </li>
        
          <li>
            <a href="/blog/2018/10/03/flume-source-code-hdfs-sink/">Flume Source Code: HDFS Sink</a>
          </li>
        
          <li>
            <a href="/blog/2018/09/20/how-to-avoid-null-pointer-exception/">How to Avoid NullPointerException</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://mirrors.creativecommons.org/presskit/buttons/80x15/svg/by-nc-sa.svg"></a>
      <br>
      &copy; 2019 Ji ZHANG<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/categories/Big-Data" class="mobile-nav-link">Big Data</a>
  
    <a href="/categories/Programming" class="mobile-nav-link">Programming</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
  <a href="http://shzhangji.com/cnblogs" class="mobile-nav-link">中文</a>
</nav>

    
<script>
  var disqus_shortname = 'jizhang';
  
  var disqus_url = 'http://shzhangji.com/blog/2018/12/08/spark-datasource-api-v2/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>